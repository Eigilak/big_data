{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3982f0",
   "metadata": {},
   "source": [
    "# Lab 08 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5fb31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1096af6",
   "metadata": {},
   "source": [
    "First, we define a function that applies value function iteration. Just exectue the code below. Don't change it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c61a8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vit(t_move,t_stay,reward,gamma):\n",
    "    '''\n",
    "    The function performs valuve function iteration. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_move : Transition probability matrix if agent decides to move -  2-d numpy array\n",
    "    t_stay : Transition probability matrix if agent decides to stay -  2-d numpy array\n",
    "    reward : Reward matrix for moving from one state to the next - 2-d numpy array\n",
    "    gamma : Discount factor - float >0 and <1 \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List with state values and optimal policy\n",
    "    \n",
    "    '''\n",
    "    v_0=np.zeros((5,5))\n",
    "    policy=[\"na\",\"na\",\"na\",\"na\",\"na\"]\n",
    "    c=100\n",
    "    ha=1\n",
    "    while c>0.0001:\n",
    "        v_c=np.copy(v_0)\n",
    "        if ha%10==0:\n",
    "            print(\"Iteration:\", ha)\n",
    "        \n",
    "        v_1_move=np.multiply(t_move,((reward+gamma*v_0))).sum(axis=1)\n",
    "        v_1_stay=np.multiply(t_stay,((reward+gamma*v_0))).sum(axis=1)\n",
    "        for i in range(0,5):\n",
    "            if v_1_move[i]>=v_1_stay[i]:\n",
    "                v_0[:,i]=v_1_move[i]\n",
    "                policy[i]=\"move\"\n",
    "            else:\n",
    "                v_0[:,i]=v_1_stay[i]\n",
    "                policy[i]=\"stay\"\n",
    "        c=np.absolute(v_c[0,:]-v_0[0,:]).sum()\n",
    "    \n",
    "    output=[v_0[0,:],policy]\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2c4f9",
   "metadata": {},
   "source": [
    "## Outline of the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320119f",
   "metadata": {},
   "source": [
    "Consider the following problem: We have an agent that wants to move from left to right. For this reason, the agent can choose \n",
    "between two actions, go (to the right) or stay. The agent starts in field A. If it chooses to go,\n",
    " it will deterministically (probability=100%) move to the next field. If it decides to stay, it will deterministically stay on the current field. The goal is to find a strategy that helps the agent move from left to right as fast as possible. \n",
    "    \n",
    " [A][B][C][D][E]\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e279dc",
   "metadata": {},
   "source": [
    "## Reward structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc8ff4",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10e72e",
   "metadata": {},
   "source": [
    "Let us now think about the reward structure:\n",
    "The agent will receive a reward of +1 only when it reaches the final state (state E). For all other transitions, the reward will be 0 (also if the agents remains in the final state). \n",
    "Please fill in the missing part in the reward matrix below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee2f77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r_0=np.array([0,0,0,0,1])\n",
    "r_1=np.array([0,0,0,0,0])\n",
    "reward=np.stack((r_0,r_0,r_0,r_0,r_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e1ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b157370",
   "metadata": {},
   "source": [
    "## Transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f4adf",
   "metadata": {},
   "source": [
    "## Question 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d149483",
   "metadata": {},
   "source": [
    "Next, we will think about the transition probabilities, i.e. we have to find the transition matrices for the two actions \"move\" and \"stay\". Please fill out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee6163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m_0=([0,1,0,0,0])\n",
    "t_m_1=([0,0,1,0,0])\n",
    "t_m_2=([0,0,0,1,0])\n",
    "t_m_3=([0,0,0,0,1])\n",
    "t_m_4=([0,0,0,0,0])\n",
    "t_move=np.stack((t_m_0,t_m_1,t_m_2,t_m_3,t_m_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72b17d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_s_0=([1,0,0,0,0])\n",
    "t_s_1=([0,1,0,0,0])\n",
    "t_s_2=([0,1,0,0,0])\n",
    "t_s_3=([0,0,1,0,0])\n",
    "t_s_4=([0,0,0,1,0])\n",
    "t_stay=np.stack((t_s_0,t_s_1,t_s_2,t_s_3,t_s_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de18ef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7892b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e587fb2",
   "metadata": {},
   "source": [
    "##  Setting a value for gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87d7e4",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a335c",
   "metadata": {},
   "source": [
    "Finally, we have to decide which discount factor we want to use. You can use every value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f761cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma=0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e95478",
   "metadata": {},
   "source": [
    "## Compute the value function and the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6635",
   "metadata": {},
   "source": [
    "## Question 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba576378",
   "metadata": {},
   "source": [
    "Now we are ready to start the value function iteration and to compute the optimal policy. Call the function vit and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a12c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=vit(t_move,t_stay,reward,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a27e2a",
   "metadata": {},
   "source": [
    "Have a look at the results. What are they telling you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10996f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67248588, 0.96074012, 1.37248588, 1.96074012, 1.37248588])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85923b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['move', 'move', 'move', 'move', 'stay']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164646ef",
   "metadata": {},
   "source": [
    "Plot the optimal policy. Does this make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f581aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdfb008df70>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([\"A\",\"B\",\"C\",\"D\",\"E\"],res[0])\n",
    "\n",
    "# Se simones eller lasses, du har ikke downloaded matplotlib så se deres resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844ec1b",
   "metadata": {},
   "source": [
    "## Question 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb8aea",
   "metadata": {},
   "source": [
    "How does the results change if we change gamma? Try different values for gamma.\n",
    "Loop over different values of gamma and store the output of the value function in a list. The result should be a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec3c3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vals=[]\n",
    "\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    v_vals.append((vit(t_move,t_stay,reward,i)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9da446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.0100e-03, 1.0100e-02, 1.0101e-01, 1.0101e+00, 1.0101e-01]),\n",
       " array([0.0083328, 0.041664 , 0.2083328, 1.041664 , 0.2083328]),\n",
       " array([0.02966838, 0.09889461, 0.32966838, 1.09889461, 0.32966838]),\n",
       " array([0.07618249, 0.19047299, 0.47618249, 1.19047299, 0.47618249]),\n",
       " array([0.16665649, 0.33331299, 0.66665649, 1.33331299, 0.66665649]),\n",
       " array([0.33746572, 0.56247943, 0.93746572, 1.56247943, 0.93746572]),\n",
       " array([0.67248588, 0.96074012, 1.37248588, 1.96074012, 1.37248588]),\n",
       " array([1.42214479, 1.77771583, 2.22214479, 2.77771583, 2.22214479]),\n",
       " array([3.83660536, 4.26294482, 4.73660536, 5.26294482, 4.73660536])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253af1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay'],\n",
       " ['move', 'move', 'move', 'move', 'stay']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_vals=[]\n",
    "\n",
    "for i in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    v_vals.append((vit(t_move,t_stay,reward,i)[1]))\n",
    "    \n",
    "v_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb51d6",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fbaf7",
   "metadata": {},
   "source": [
    "Now we extend our state space. Instead of moving from point A to point E, the agent should now move from A to J (=there are 10 different states).\n",
    "Look at the function vit and change it accordingly.\n",
    "\n",
    " [A][B][C][D][E][F][G][H][I][J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "117bc670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_1(t_move,t_stay,t_back,reward,gamma):\n",
    "    '''\n",
    "    The function performs valuve function iteration. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_move : Transition probability matrix if agent decides to move -  2-d numpy array\n",
    "    t_stay : Transition probability matrix if agent decides to stay -  2-d numpy array\n",
    "    reward : Reward matrix for moving from one state to the next - 2-d numpy array\n",
    "    gamma : Discount factor - float >0 and <1 \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List with state values and optimal policy\n",
    "    \n",
    "    '''\n",
    "    v_0=np.zeros((10,10))\n",
    "    policy=[\"na\",\"na\",\"na\",\"na\",\"na\",\"na\",\"na\",\"na\",\"na\",\"na\"]\n",
    "    c=100\n",
    "    ha=1\n",
    "    while c>0.0001:\n",
    "        v_c=np.copy(v_0)\n",
    "        if ha%10==0:\n",
    "            print(\"Iteration:\", ha)\n",
    "        \n",
    "        v_1_move=np.multiply(t_move,((reward+gamma*v_0))).sum(axis=1)\n",
    "        v_1_stay=np.multiply(t_stay,((reward+gamma*v_0))).sum(axis=1)\n",
    "        v_1_back=np.multiply(t_back,((reward+gamma*v_0))).sum(axis=1)\n",
    "\n",
    "        for i in range(0,10):\n",
    "            if v_1_move[i]>=v_1_stay[i] and v_1_move[i]>=v_1_back[i]:\n",
    "                v_0[:,i]=v_1_move[i]\n",
    "                policy[i]=\"move\"\n",
    "            elif v_1_back[i]>=v_1_stay[i]:\n",
    "                v_0[:,i]=v_1_back[i]\n",
    "                policy[i]=\"back\"\n",
    "            else:\n",
    "                v_0[:,i]=v_1_stay[i]\n",
    "                policy[i]=\"stay\"\n",
    "        c=np.absolute(v_c[0,:]-v_0[0,:]).sum()\n",
    "    \n",
    "    output=[v_0[0,:],policy]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fd49c",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8802c",
   "metadata": {},
   "source": [
    "Unfortunately, we also have to change our reward structure. Please change it according to the new state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c6e0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0=np.array([0,0,0,0,0,0,0,0,0,1])\n",
    "r_1=np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "reward=np.stack((r_0,r_0,r_0,r_0,r_0,r_0,r_0,r_0,r_0,r_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ca338c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a0242",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5b590",
   "metadata": {},
   "source": [
    "Please also modify the transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92cf3140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_m_0=np.array([0,1,0,0,0,0,0,0,0,0])\n",
    "t_m_1=np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "t_m_2=np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "t_m_3=np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "t_m_4=np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "t_m_5=np.array([0,0,0,0,0,0,1,0,0,0])\n",
    "t_m_6=np.array([0,0,0,0,0,0,0,1,0,0])\n",
    "t_m_7=np.array([0,0,0,0,0,0,0,0,1,0])\n",
    "t_m_8=np.array([0,0,0,0,0,0,0,0,0,1])\n",
    "t_m_9=np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "t_move=np.stack((t_m_0,t_m_1,t_m_2,t_m_3,t_m_4,t_m_5,t_m_6,t_m_7,t_m_8,t_m_9))\n",
    "\n",
    "\n",
    "t_s_0=np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "t_s_1=np.array([0,1,0,0,0,0,0,0,0,0])\n",
    "t_s_2=np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "t_s_3=np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "t_s_4=np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "t_s_5=np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "t_s_6=np.array([0,0,0,0,0,0,1,0,0,0])\n",
    "t_s_7=np.array([0,0,0,0,0,0,0,1,0,0])\n",
    "t_s_8=np.array([0,0,0,0,0,0,0,0,1,0])\n",
    "t_s_9=np.array([0,0,0,0,0,0,0,0,0,1])\n",
    "\n",
    "t_stay=np.stack((t_s_0,t_s_1,t_s_2,t_s_3,t_s_4,t_s_5,t_s_6,t_s_7,t_s_8,t_s_9))\n",
    "\n",
    "t_b_0=np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "t_b_1=np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "t_b_2=np.array([0,1,0,0,0,0,0,0,0,0])\n",
    "t_b_3=np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "t_b_4=np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "t_b_5=np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "t_b_6=np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "t_b_7=np.array([0,0,0,0,0,0,1,0,0,0])\n",
    "t_b_8=np.array([0,0,0,0,0,0,0,1,0,0])\n",
    "t_b_9=np.array([0,0,0,0,0,0,0,0,1,0])\n",
    "\n",
    "t_back=np.stack((t_b_0,t_b_1,t_b_2,t_b_3,t_b_4,t_b_5,t_b_6,t_b_7,t_b_8,t_b_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b9f2983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1623a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "381d9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7896a2e",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41514e73",
   "metadata": {},
   "source": [
    "Compute the optimal policy. Call the function vit_1 using the new reward and transition matrices and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca0a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1=vit_1(t_move,t_stay,t_back,reward,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a67f4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11301366, 0.16146386, 0.23066266, 0.32953386, 0.47076266,\n",
       "       0.67253386, 0.96076266, 1.37253386, 1.96076266, 1.37253386])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6de8c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'back']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc67a0b",
   "metadata": {},
   "source": [
    "## Question 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89df491",
   "metadata": {},
   "source": [
    "Can you think about a more complicated problem? For instance, you could allow the agent to move back; change the reward structure; or change the state space. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7059149",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bf050-287a-4ae7-b9ac-a7a1a5d88227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7036c2-0da0-4d58-9d4c-089220dc13e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc104fd-c4c7-4e4e-84fc-c6ba0fc12f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
