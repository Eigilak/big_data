{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfd6554",
   "metadata": {},
   "source": [
    "# Lab 08 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f79ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cec2e",
   "metadata": {},
   "source": [
    "First, we define a function that applies value function iteration. Just exectue the code below. Don't change it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3f8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit(t_move,t_stay,reward,gamma):\n",
    "    '''\n",
    "    The function performs valuve function iteration. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_move : Transition probability matrix if agent decides to move -  2-d numpy array\n",
    "    t_stay : Transition probability matrix if agent decides to stay -  2-d numpy array\n",
    "    reward : Reward matrix for moving from one state to the next - 2-d numpy array\n",
    "    gamma : Discount factor - float >0 and <1 \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List with state values and optimal policy\n",
    "    \n",
    "    '''\n",
    "    v_0=np.zeros((5,5))\n",
    "    policy=[\"na\",\"na\",\"na\",\"na\",\"na\"]\n",
    "    c=100\n",
    "    ha=1\n",
    "    while c>0.0001:\n",
    "        v_c=np.copy(v_0)\n",
    "        if ha%10==0:\n",
    "            print(\"Iteration:\", ha)\n",
    "        \n",
    "        v_1_move=np.multiply(t_move,((reward+gamma*v_0))).sum(axis=1)\n",
    "        v_1_stay=np.multiply(t_stay,((reward+gamma*v_0))).sum(axis=1)\n",
    "        for i in range(0,5):\n",
    "            if v_1_move[i]>=v_1_stay[i]:\n",
    "                v_0[:,i]=v_1_move[i]\n",
    "                policy[i]=\"move\"\n",
    "            else:\n",
    "                v_0[:,i]=v_1_stay[i]\n",
    "                policy[i]=\"stay\"\n",
    "        c=np.absolute(v_c[0,:]-v_0[0,:]).sum()\n",
    "    \n",
    "    output=[v_0[0,:],policy]\n",
    "    \n",
    "    return output\n",
    "                    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74735f4",
   "metadata": {},
   "source": [
    "## Outline of the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f4ddf",
   "metadata": {},
   "source": [
    "Consider the following problem: We have an agent that wants to move from left to right. For this reason, the agent can choose \n",
    "between two actions, go (to the right) or stay. The agent starts in field A. If it chooses to go,\n",
    " it will deterministically (probability=100%) move to the next field. If it decides to stay, it will deterministically stay on the current field. The goal is to find a strategy that helps the agent move from left to right as fast as possible. \n",
    "    \n",
    " [A][B][C][D][E]\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad48539",
   "metadata": {},
   "source": [
    "## Reward structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4baeded",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b16c7",
   "metadata": {},
   "source": [
    "Let us now think about the reward structure:\n",
    "The agent will receive a reward of +1 only when it reaches the final state (state E). For all other transitions, the reward will be 0 (also if the agents remains in the final state). \n",
    "Please fill in the missing part in the reward matrix below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3d2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0=np.array(['0'])\n",
    "r_1=np.array(['1'])\n",
    "reward=np.stack((r_0,r_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ad81fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0'],\n",
       "       ['1']], dtype='<U1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a9c3d",
   "metadata": {},
   "source": [
    "## Transition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c8b01",
   "metadata": {},
   "source": [
    "## Question 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd53dc",
   "metadata": {},
   "source": [
    "Next, we will think about the transition probabilities, i.e. we have to find the transition matrices for the two actions \"move\" and \"stay\". Please fill out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f88b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "move = 'move'\n",
    "t_m_0=([move])\n",
    "t_m_1=([move])\n",
    "t_m_2=([move])\n",
    "t_m_3=([move])\n",
    "t_m_4=([move])\n",
    "t_move=np.stack((t_m_0,t_m_1,t_m_2,t_m_3,t_m_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b47bee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e5a6c03b7688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_s_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt_s_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt_s_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt_s_3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt_s_4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
     ]
    }
   ],
   "source": [
    "stay = 'stay'\n",
    "t_s_0=([stay])\n",
    "t_s_1=([stay])\n",
    "t_s_2=([_____])\n",
    "t_s_3=([_____])\n",
    "t_s_4=([_____])\n",
    "t_stay=np.stack((_____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ddded",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73c203",
   "metadata": {},
   "source": [
    "##  Setting a value for gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613e23e",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26840135",
   "metadata": {},
   "source": [
    "Finally, we have to decide which discount factor we want to use. You can use every value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6770ca",
   "metadata": {},
   "source": [
    "## Compute the value function and the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb0eff",
   "metadata": {},
   "source": [
    "## Question 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c25ac7",
   "metadata": {},
   "source": [
    "Now we are ready to start the value function iteration and to compute the optimal policy. Call the function vit and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=vit(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f902eb",
   "metadata": {},
   "source": [
    "Have a look at the results. What are they telling you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15218c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8756745",
   "metadata": {},
   "source": [
    "Plot the optimal policy. Does this make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([\"A\",\"B\",\"C\",\"D\",\"E\"],res[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b8420",
   "metadata": {},
   "source": [
    "## Question 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5513b6c",
   "metadata": {},
   "source": [
    "How does the results change if we change gamma? Try different values for gamma.\n",
    "Loop over different values of gamma and store the output of the value function in a list. The result should be a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vals=[]\n",
    "\n",
    "for i in [_____________]:\n",
    "    v_vals.append(__________)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7861fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded4e22",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c81750",
   "metadata": {},
   "source": [
    "Now we extend our state space. Instead of moving from point A to point E, the agent should now move from A to J (=there are 10 different states).\n",
    "Look at the function vit and change it accordingly.\n",
    "\n",
    " [A][B][C][D][E][F][G][H][I][J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41843775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_1(t_move,t_stay,reward,gamma):\n",
    "    '''\n",
    "    The function performs valuve function iteration. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_move : Transition probability matrix if agent decides to move -  2-d numpy array\n",
    "    t_stay : Transition probability matrix if agent decides to stay -  2-d numpy array\n",
    "    reward : Reward matrix for moving from one state to the next - 2-d numpy array\n",
    "    gamma : Discount factor - float >0 and <1 \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List with state values and optimal policy\n",
    "    \n",
    "    '''\n",
    "    v_0=np.zeros((_,_))\n",
    "    policy=[_,_,_,_,_,_,_,_,_,_]\n",
    "    c=100\n",
    "    ha=1\n",
    "    while c>0.0001:\n",
    "        v_c=np.copy(v_0)\n",
    "        if ha%10==0:\n",
    "            print(\"Iteration:\", ha)\n",
    "        \n",
    "        v_1_move=np.multiply(t_move,((reward+gamma*v_0))).sum(axis=1)\n",
    "        v_1_stay=np.multiply(t_stay,((reward+gamma*v_0))).sum(axis=1)\n",
    "        for i in range(0,_):\n",
    "            if v_1_move[i]>=v_1_stay[i]:\n",
    "                v_0[:,i]=v_1_move[i]\n",
    "                policy[i]=\"move\"\n",
    "            else:\n",
    "                v_0[:,i]=v_1_stay[i]\n",
    "                policy[i]=\"stay\"\n",
    "        c=np.absolute(v_c[0,:]-v_0[0,:]).sum()\n",
    "    \n",
    "    output=[v_0[0,:],policy]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f6cd5",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9737b",
   "metadata": {},
   "source": [
    "Unfortunately, we also have to change our reward structure. Please change it according to the new state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0=np.array([_____])\n",
    "r_1=np.array([_____])\n",
    "reward=np.stack((_____))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6429ba2",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6867c3",
   "metadata": {},
   "source": [
    "Please also modify the transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m_0=np.array([__________])\n",
    "t_m_1=np.array([__________])\n",
    "t_m_2=np.array([__________])\n",
    "t_m_3=np.array([__________])\n",
    "t_m_4=np.array([__________])\n",
    "t_m_5=np.array([__________])\n",
    "t_m_6=np.array([__________])\n",
    "t_m_7=np.array([__________])\n",
    "t_m_8=np.array([__________])\n",
    "t_m_9=np.array([__________])\n",
    "\n",
    "t_move=np.stack((__________))\n",
    "\n",
    "\n",
    "t_s_0=np.array([__________])\n",
    "t_s_1=np.array([__________])\n",
    "t_s_2=np.array([__________])\n",
    "t_s_3=np.array([__________])\n",
    "t_s_4=np.array([__________])\n",
    "t_s_5=np.array([__________])\n",
    "t_s_6=np.array([__________])\n",
    "t_s_7=np.array([__________])\n",
    "t_s_8=np.array([__________])\n",
    "t_s_9=np.array([__________])\n",
    "t_stay=np.stack((__________))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8119de",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0050e72",
   "metadata": {},
   "source": [
    "Compute the optimal policy. Call the function vit_1 using the new reward and transition matrices and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1=vit(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9483a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eceacc",
   "metadata": {},
   "source": [
    "## Question 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c90cc",
   "metadata": {},
   "source": [
    "Can you think about a more complicated problem? For instance, you could allow the agent to move back; change the reward structure; or change the state space. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585b1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
